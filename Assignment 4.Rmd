---
title: "Assignment 4 Submission"
author: "Alexander Karr"
date: "2024-03-22"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
---


# Problem 1 (Easy)

## Part a:
Provide a symbolic formula that defines a causal polymorphism AND provide a definition of a causal polymorphism in words. Use no more than two total sentences for your answer.

**Symbolic formula:**

**<u>Symolic formula:</u>**
$$
A_1 \rightarrow A_2 \Rightarrow \Delta Y \mid Z
$$
**<u>Definition in words:</u>**

**A particular position in the genome - for which more than one allele exists amongst a specified population - where manipulations of the DNA, under specifiable conditions, would produce an effect on a specified phenotype.** 

## Part b: 
Consider a case where we have rejected the null hypothesis for a genotype in a GWAS analysis. What are reasons why this genotype may NOT indicate the position of a causal polymorphism (provide two reasons total)?

**<u>Reason 1 - Correlated SNPs:</u>**

**Because of how proximately located SNPs on the same chromosome are less likely to be separated by recombination during meiosis, causal SNPs are usually surrounded by correlated SNPs. It would be very easy to mistake one of such SNP's correlation to a causal polymorphism for causality, itself.**

**<u>Reason 2 - Pure chance, made worse by the multiple testing problem:</u>**

**Because any given p-value represents the probability of witnessing a given value of your test statistic (or a more extreme value), conditional on the null being true, no matter how low an $\alpha$ you set, the probability will never equal $0$. Thus, no matter how low the p-value, there exists *that* probability that, with the null distribution being the true distribution, you would have obtained the test statistic that you did. Additionally, multiple testing in GWAS studies - the fact that you are testing causality for hundreds to thousands to millions of SNPs - means that you can expect to see a much large number of those "by-chance" p-values by simple virtue of the staggering number of tests you are conducting.** 

# Problem 2 (Medium)
Consider the data in the files (‘QG24phenotypes.txt’; ‘QG24genotypes.txt’) of scaled height phenotypes and SNP genotype data (respectively) collected in a GWAS. Note that in the ‘phenotypes’ file the column lists the individuals in order (1st entry is the phenotype for individual 1, the nth is for individual $n$). Also note that for each of the SNPs, there are two total alleles, i.e. two letters for each SNP and there are three possible states per SNP genotype: two homozygotes and a heterozygote. In the “genotypes” file, each column represents a specific SNP (column 1 = genotype 1, column 2 = genotype 2) and each consecutive pair of rows represent all of the genotype states for an individual for the entire set of SNPs (rows 1 and 2 = all of individual 1’s genotypes, rows 3 and 4 = all individual 2’s genotypes). Also note that the genotypes in the file are listed in order along the genome such that the first genotype is ‘genotype 1’ and the last is ‘genotype N ’.

**I load all relevant libraries.**
```{r}
library(MASS)
```



## Part a:
Write code that inputs the phenotype, plus an (additional) line of code that calculates the number of samples n and report the number n (NOTE: you do not have to output the phenotypes (!!) just provide the R code and report the value for n.

```{r}
phenotypes = readLines("QG24phenotypes.txt")
num_samples = length(phenotypes)
print(paste0("The number of scaled height phenotype samples is ", num_samples))
```


## Part b:
Write code that produces a histogram of your phenotype data (NOTE: provide the R code and the histogram).

```{r}
# Convert phenotypes to numeric
phenotypes_numeric = as.numeric(phenotypes)

# Make a histogram of the phenotype values
hist(phenotypes_numeric, main="Histogram of Phenotype Values", xlab="Phenotype Values", col="blue")
```


## Part c:
Write code that inputs the genotype data plus a line of code that outputs the number of genotypes N and sample size n and report these numbers (NOTE: that you do not have to output the genotypes (!!) just provide the R code and report the value for N and n you obtained from these data).

```{r}
  # Step 1: Read the genotype data from a file
genotypes <- read.table("QG24genotypes.txt", header = FALSE, stringsAsFactors = FALSE)
print(head(genotypes))
```
```{r}
print(dim(genotypes))
```
**The dimensions of the genotype dataframe are 400 x 1000. So there are 1,000 separate genotypes (with two alleles for each SNP being measured across people. As for sample size, there are 400 separate rows of genotypes, but given that each individual is represented by two rows, there are two separate samples of complete genotypes for people.**

**Given that the $x_a$ and $x_d$ matrices both intake two rows, and output one row, we would expect them to have half as many rows as the original genotypes dataframe later on. This would be 200 rows of genotypes, for each of the two-hundred phenotype samples.**

## Part d:
Write code that converts your genotype data input in part [c] into two new matrices, the first a matrix where each genotype is converted to the appropriate Xa value and the second where each genotype is converted to the appropriate Xd value (NOTE: that you do not have to output the matrices (!!) just provide the R code that will create the matrices if we run it).

```{r}
genotype_coder_row_based <- function(geno_import, maf_limit, error_value = 3){
  
  # Combines pairs of rows for each SNP into one double long vector
  geno_input <- mapply(c, geno_import[seq(1, nrow(geno_import), 2), ], geno_import[seq(2, nrow(geno_import), 2), ], SIMPLIFY = FALSE)
  
  xa_converter <- function(geno_rows, numAlleles, maf_limit){
    # Flatten the list of allele rows into a single vector
    geno_col <- unlist(geno_rows)
    # Count alleles
    geno_count <- table(geno_col) 
    # MAF filtering
    if(min(geno_count)/length(geno_col) <= maf_limit | length(geno_count) < 2){
      return(rep(error_value, numAlleles/2)) 
    }
    minor_allele <- names(geno_count[geno_count == min(geno_count)])
    # Convert to 0, 1, 2 coding
    xa <- (geno_col[1:(numAlleles/2)]==minor_allele) + (geno_col[((numAlleles/2)+1):numAlleles]==minor_allele)
    xa <- xa-1
    return(xa)
  }
  
  # Apply xa_converter to the input by column, with numAlleles=nrow(data), and maf_limit=0.05
  xa_mat <- sapply(geno_input, xa_converter, numAlleles = nrow(geno_import), maf_limit = 0.05)
  
  # Filter out error values and keep track of the original indices
  valid_indices <- which(xa_mat[1, ] != error_value)
  xa_mat_filtered <- xa_mat[, valid_indices]
  
  # Dominance coding
  xd_mat <- 1 - 2*abs(xa_mat_filtered)
  
  # Mapping of retained SNPs to their original positions
  original_position_mapping <- valid_indices
  
  return(list(xa_mat = xa_mat_filtered, xd_mat = xd_mat, original_position_mapping = original_position_mapping))
}


```

```{r}
# Convert alleles to dummy variable coding and save in a list
  codes <- genotype_coder_row_based(genotypes, 0)
# Get each specific coding from the list
  xa_mat <- codes[[1]]
  xd_mat <- codes[[2]]
  original_position_mapping <- codes[[3]]
# Show the coding
  xa_mat[1:10,1:10]
```



```{r}
xd_mat[1:10,1:10]
```

```{r}
print("The dimensions of the X_a and X_d matrices are as follows, respectively:")
print(dim(xa_mat))
print(dim(xd_mat))
```
**Because of my MAF filtering, or my filtering of any SNPs for which there are more than two alleles, my $X_{A}$ and $X{D}$ emerge with fewer loci than existed in the original genotypes file. This is why I kept a mapping of the original SNP positions to their new position post-filtering in my $X_{A}$ and $X_{D}$ matrices, such that I can correctly trace back downstream p-values to original genomic positions.**

**We observed the following formula to calculate our $\hat{β}$ matrix:**
$$
\hat{\boldsymbol{\beta}}_{\text{MLE}} = (\boldsymbol{X}^\top \boldsymbol{X})^{-1} \boldsymbol{X}^\top \boldsymbol{y}
$$
**where**
$$
y = \begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}
$$
**and**
$$
x = \begin{bmatrix}
1 & x_{1,a} & x_{1,d} \\
1 & x_{2,a} & x_{2,d} \\
\vdots & \vdots & \vdots \\
1 & x_{n,a} & x_{n,d}
\end{bmatrix}
$$

## Part e:
Write code to calculate $MLE(\hat{β}ˆ) = [\hat{β}_{u}, \hat{β}_{a}, \hat{β}_{d}]$ for each genotype in the dataset, an F-statistic for each genotype, and a p-value for each genotype using the R function pf(F-statistic, df1, df2, lower.tail = FALSE). PLEASE NOTE (!!): that you may NOT use an existing R function for ANY of these calculations other than the calculation of the p-value (=you must write code that calculates each component except the p-value) and NOTE: you do not have to output anything (!!) just provide the R code.

```{r}
# Assuming 'X_a' and 'X_d' are your matrices and 'phenotypes_df' is your dataframe of phenotypes
# Convert the dataframe of phenotypes to a numeric matrix
y <- as.matrix(phenotypes_numeric)

# Make sure that the phenotypes matrix is a column vector
if (is.vector(y)) {
    y <- matrix(y, ncol = 1)
}
print(head(y))
```
```{r}
pval_calculator <- function(pheno_input, xa_input, xd_input){
    # Combine the data for the beta estimates
    X_mx <- cbind(1,xa_input,xd_input)
    # Estimate the beta values
    MLE_beta <- ginv(t(X_mx) %*% X_mx) %*% t(X_mx) %*% pheno_input
    # Estimate y_hat
    y_hat <- X_mx %*% MLE_beta
    
    # Calculate SSM,SSE,MSM,MSE,F-statistic, and p-value
    SSM <- sum((y_hat - mean(pheno_input))^2)
    SSE <- sum((pheno_input - y_hat)^2)
    df_M <- 2
    df_E <- num_samples - 3 
    MSM <- SSM / df_M
    MSE <- SSE / df_E
    Fstatistic <- MSM / MSE
    pval <- pf(Fstatistic, df_M, df_E,lower.tail = FALSE)
    return(pval)
}

# Run this function across the loci in our genotype data
pval_mx <- rep(0,ncol(xa_mat))
for(i in 1:ncol(xa_mat)){
  pval_mx[i] <- pval_calculator(y[,1], xa_mat[,i], xd_mat[,i])
}
# How's it look?
pval_mx[1:10]
```


## Part f:
Write code to produce a Manhattan plot (i.e., genotypes in order on the x-axis and -log(pvalues) on the y-axis. PLEASE NOTE (!!): do NOT use an R function (=write your own code to produce the Manhattan plot) but DO provide your code AND your Manhattan plot.
```{r}
library(tidyverse)
```


```{r}
# Make a simple manhattan plot
plot_df <- data.frame(index = 1:length(pval_mx), pval = pval_mx)
ggplot(plot_df, aes(index, -log10(pval_mx))) + geom_point()

```


## Part g:
Write code to produce a Quantile-Quantile (QQ) plot for your p-values PLEASE NOTE (!!): do NOT use an R function (=write your own code to produce the Manhattan plot) but DO provide your code AND your QQ plot.

```{r}
# Step 1: Sort the p-values in ascending order
sorted_pval_mx <- sort(pval_mx)

# Step 2: Calculate expected quantiles for a uniform [0, 1] distribution
n <- length(sorted_pval_mx)
expected_quantiles <- (1:n) / (n + 1)

# Step 3: Observed quantiles are the sorted p-values themselves
observed_quantiles <- sorted_pval_mx

# Step 4: Plot the QQ plot
plot(expected_quantiles, observed_quantiles, xlab = "Expected Quantiles", ylab = "Observed Quantiles", main = "QQ Plot of P-Values")
abline(0, 1, col = "red") # This adds the y=x line for reference

# Optional: Enhance the plot
points(expected_quantiles, observed_quantiles, pch = 19, col = "blue")

```


## Part h:
Do you consider the QQ plot to indicate that you have ‘good’ model fit in this case (and can therefore interpret the results of your analysis)? Explain your reasoning using no more than two sentences.

**With the reference $y = x$ line representing the scenario where p-values from the GWAS perfectly match the expectation under the null hypothesis, the slight deviation to the right of points at the lower tail indicate that we are observing more low p-values than would be expected by chance. This distinction remains from p-values of about 0 to 0.8 rather than just being for a very short p-value interval, further communicating that the true underlying distribution may be something other than the null, rather than us just observing some outlier, by-chance low p-values.**


## Part i:
Write code that uses a Bonferroni correction to produce an overall study controlled Type I error of 0.05 to assess whether to reject the null hypothesis for each genotype, where your code also outputs the number of each genotype for which you rejected the null (remember: the genotypes are provided in order along the genome!). Report the numbers of all genotypes for which you rejected the null.

**We calculate our Bonferroni adjusted p-value for $n$ number of performed tests:**

$$
p_{adj} = \frac{0.05}{n}
$$
**and reject the null hypothesis only if the adjusted p value for a locus is below this value.**
```{r}

alpha <- 0.05
n <- length(pval_mx)
p_adj <- alpha / n

# Catalogue the indices of all entries with p-values below p_adj
significant_indices <- which(pval_mx < p_adj)

# Map indices back to their original genomic positions
significant_positions <- original_position_mapping[significant_indices]

# Output the number of p-values for which we now adjustably reject the null
num_significant <- length(significant_indices)

# Report the numbers (indices) of each genotype for which you rejected the null
cat("Number of p-values below the adjusted threshold:", num_significant, "\n")
cat("Genomic positions of significant p-values (genotypes):", toString(significant_positions), "\n")



```


## Part j:
Assuming the set of genotypes for which you rejected the null hypothesis in part [i] do indeed indicate the positions of causal genotypes in the genome, how many causal genotypes do you think these significant genotypes are indicating overall? Explain your reasoning using no more than two sentences.

**I would guess that there are two causal genotypes. Given that proximate SNPs are correlated, and that that correlation usually declines with increasing distance, the fact that we have two notable clusters of significant SNPs ([116, 195] and [873, 878] inclusive) that are both dense and separated by a region demonstrating only one significant genotype (index 562), we have decent evidence to predict two causal signal, driving correlated signal in their close genomic surroundings.**


# Problem 3 (Difficult)
Consider the form of the F-statistic for the genetic regression model (which may include covariates):
$$
F_{[2, n-3]} = \frac{\frac{SSE(\hat{\theta}_{0}) - SSE(\hat{\theta}_{1})}{2}}{\frac{SSE(\hat{\theta}_{1})}{n-\#(\hat{\theta}_{1})}}
$$
and by noting that the likelihood of a linear regression model has the form:
$$
L(\beta, \sigma_{\varepsilon}^2 \mid \mathbf{x}, \mathbf{y}) = \frac{\frac{1}{\left(\sqrt{2\pi\sigma^2\varepsilon}\right)^n
} e^{-\frac{1}{2\sigma_{\varepsilon}^2} \sum (y_i - \hat{\beta}_u)^2}}{\frac{1}{\left(\sqrt{2\pi\sigma^2\varepsilon}\right)^n
} e^{-\frac{1}{2\sigma_{\varepsilon}^2} \sum (y_i - \hat{y}_i)^2}}
$$
where $β$ in this equation is the vector of $β$ parameters, the $x$ is the observed matrix of independent variables, $y$ is the observed vector of dependent variables, and $\hat{y}_{i}$ is the predicted phenotype value for individual $i$ for the appropriate regression model, show that:
$$
\left( \frac{2}{n - \#(\hat{\theta}_1)} F_{2,n-\#(\hat{\theta}_1)} + 1 \right)^{\frac{n}{2}} = \Lambda
$$
where $Λ$ is the Likelihood Ratio statistic for the genetic regression model (note that this is just $Λ$ not $−2lnΛ$!), i.e. show that a simple transformation of the F-statistic produces the $LR$ (where this fact can be used to show the F-statistic shares desirable properties of the Likelihood Ratio Test statistic $LRT = −2lnΛ$, although note the F-statistic and $LRT$ test statistic have different distributions when the null hypothesis is true!)

*Before, I go about solving the problem, I must include a short preface that I find myself confused by. The likelihood equation in equation (2) on the problem set itself is listed as follows:*
$$
\frac{1}{(2\pi)^{\frac{n}{2}} (\sigma_{\varepsilon})^n} e^{-\frac{1}{2\sigma_{\varepsilon}^2} \sum (y_i - \hat{y}_i)^2}
$$

*However, the formula that Jason uses during office hours, which I find myself requiring to be able to continue with the proof, is the different one that I listed originally (with the entirety of the denominator being to the $\frac{n}{2}$ power). I will use this formula, but felt that I needed to specify this.*


**I will begin by demonstrating how a modification of the F-statistic formula produces the term raised to the exponent in Equation (3).**


$$
F_{[2, n-3]} = \frac{\frac{SSE(\hat{\theta}_{0}) - SSE(\hat{\theta}_{1})}{2}}{\frac{SSE(\hat{\theta}_{1})}{n-\#(\hat{\theta}_{1})}} \rightarrow \frac {2}{n-\#(\hat{\theta}_{1})} F_{[2, n-3]} = \frac{SSE(\hat{\theta}_{0}) - SSE(\hat{\theta}_{1})}{SSE(\hat{\theta}_{1})} \rightarrow \frac {2}{n-\#(\hat{\theta}_{1})} F_{[2, n-3]} = \frac{SSE(\hat{\theta}_{0})}{SSE(\hat{\theta}_{1})} - 1 \rightarrow \frac {2}{n-\#(\hat{\theta}_{1})} F_{[2, n-3]} + 1 = \frac{SSE(\hat{\theta}_{0})}{SSE(\hat{\theta}_{1})}
$$

**We now know that $\frac {2}{n-\#(\hat{\theta}_{1})} F_{[2, n-3]} + 1$, the term raised to the exponent in Equation (3), is equivalent to $\frac{SSE(\hat{\theta}_{0})}{SSE(\hat{\theta}_{1})}$**. 

**Therefore, plugging that into equation (3), we now know the following equality, and will seek to prove it:**
$$
(\frac{SSE(\hat{\theta}_{0})}{SSE(\hat{\theta}_{1})})^{-\frac{n}{2}} = \lambda
$$

**To demonstrate that a simple transformation of the F-statistic produces the likelihood ratio, we begin with the likelihood ratio itself:**

$$
\lambda = \frac{L(\beta, \sigma_{\varepsilon}^2 \mid \mathbf{x}, \mathbf{y}, \mathbf{H_{0}})}{L(\beta, \sigma_{\varepsilon}^2 \mid \mathbf{x}, \mathbf{y}, \mathbf{H_{A}})} = 
\frac{\frac{1}{\left(\sqrt{2\pi\sigma^2\varepsilon}\right)^n
} e^{-\frac{1}{2\sigma_{\varepsilon}^2} \sum (y_i - \hat{\beta}_u)^2}}{\frac{1}{\left(\sqrt{2\pi\sigma^2\varepsilon}\right)^n
} e^{-\frac{1}{2\sigma_{\varepsilon}^2} \sum (y_i - \hat{y}_i)^2}}
$$

**In the null hypothesis, we know that all predicted phenotypes $\hat{y}_{i} = \hat{\beta}_{u}$, because the null assumes $(\beta_{A} = 0 \land \beta_{D} = 0)$. We focus on the exponent term first/ The process is the same for both the likelihoods of the null and the alternative. I will go through it for the null, however.** 

**The formula for the MLE of $\sigma_{\varepsilon}^2$ is as follows:**

$$
MLE(\sigma_{\varepsilon}^2) = \frac{1}{n}*\sum_{i}{}(y_i-\beta_u)^2 
$$

**Plugging this into the null likelihood, the exponent simplifies in the following fashion:**
$$
-\frac{1}{2\sigma_{\varepsilon}^2} \sum (y_i - \hat{\beta}_u)^2 = -\frac{\sum (y_i - \hat{\beta}_u)^2}{\frac{2}{n}\sum (y_i - \hat{\beta}_u)^2} = -\frac{1}{\frac{2}{n}} = -\frac{n}{2}
$$
**So our updated LR is as follows:**
$$
\frac{\frac{1}{\left(\sqrt{2\pi\sigma^2\varepsilon}\right)^n
} e^{-\frac{1}{2\sigma_{\varepsilon}^2} \sum (y_i - \hat{\beta}_u)^2}}{\frac{1}{\left(\sqrt{2\pi\sigma^2\varepsilon}\right)^n
} e^{-\frac{1}{2\sigma_{\varepsilon}^2} \sum (y_i - \hat{y}_i)^2}} = 
\frac{\frac{1}{\left(\sqrt{2\pi\sigma^2\varepsilon}\right)^n
} e^{-\frac{n}{2}}}
{\frac{1}{\left(\sqrt{2\pi\sigma^2\varepsilon}\right)^n
} e^{-\frac{n}{2}}}
$$
**We can rewrite this as:**

$$
\frac{\frac{1}{\left(\sqrt{2\pi\sigma^2\varepsilon}\right)^n
} e^{-\frac{n}{2}}}
{\frac{1}{\left(\sqrt{2\pi\sigma^2\varepsilon}\right)^n
} e^{-\frac{n}{2}}} 
=
\frac{\frac{1}{\left(\sigma^2_{\epsilon} \mid H_0\right)^{\frac{n}{2}}
}} {\frac{1}{\left(\sigma^2_{\epsilon} \mid H_A\right)^{\frac{n}{2}}
}}
= 
\frac{\left(\sigma^2_{\epsilon} \mid H_0\right)^{-\frac{n}{2}}} {\left(\sigma^2_{\epsilon} \mid H_A\right)^{-\frac{n}{2}}}
=
(\frac{\left(\sigma^2_{\epsilon} \mid H_0\right)} {\left(\sigma^2_{\epsilon} \mid H_A\right)})^{-\frac{n}{2}}
$$
**Revisiting the MLE statistics for our error terms, we can convert this into a ratio of sum of squared errors.**
$$
(\frac{\left(\sigma^2_{\epsilon} \mid H_0\right)} {\left(\sigma^2_{\epsilon} \mid H_A\right)})^{-\frac{n}{2}} = (\frac{\sum (y_i - \hat{\beta}_u)^2} {\sum (y_i - \hat{y}_i)^2})^{-\frac{n} {2}}
=
\frac{SSE(\theta_{0})}{SSE(\theta_{1})}
=
\lambda
$$

**Since demonstrating from equation (1) that $\frac {2}{n-\#(\hat{\theta}_{1})} F_{[2, n-3]} + 1 = \frac{SSE(\hat{\theta}_{0})}{SSE(\hat{\theta}_{1})}$, knowing that $\frac{SSE(\theta_{0})}{SSE(\theta_{1})} =\lambda$, we can substitute to prove equation (3).**

**Thus,**
$$
\frac {2}{n-\#(\hat{\theta}_{1})} F_{[2, n-3]} + 1 = \lambda
$$

